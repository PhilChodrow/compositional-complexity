
% -----------------------------------------------------------------------------
% ------------------------------- PREAMBLE ------------------------------------
% -----------------------------------------------------------------------------
	\documentclass[english]{scrartcl}

	\title{Spatial Compositional Complexity}
	\subtitle{v0.1}
	\author{Phil Chodrow}
	\date{March 29th, 2016}

	\usepackage[T1]{fontenc}
	\usepackage[utf8]{inputenc}
	\usepackage{babel}
	\usepackage{blindtext}
	\usepackage{amsmath}
	\usepackage{amsthm}
	\usepackage{amsfonts}
	\usepackage{amssymb}

	\setkomafont{disposition}{\normalfont\bfseries}

	\newtheorem{thm}{Theorem}
	\newtheorem{lm}{Lemma}
	\newtheorem{cor}{Corrolary}
	\newtheorem{clm}{Claim}
	\newtheorem*{thm*}{Theorem}
	\newtheorem*{lm*}{Lemma}
	\newtheorem*{cor*}{Corrolary}
	\newtheorem*{clm*}{Claim}

	\newcommand\abs[1]{\left|#1\right|}
	\newcommand\E[0]{\mathbb{E}}
	\newcommand\R[0]{\mathbb{R}}
	\newcommand\decision[2]{\underset{#2}{\overset{#1}{\gtreqless}}}
	\newcommand{\argmax}{\operatornamewithlimits{argmax}}
	\newcommand{\argmin}{\operatornamewithlimits{argmin}}
	\newcommand\prob[0]{\mathbb{P}}


% -----------------------------------------------------------------------------
% --------------------------------- BODY --------------------------------------
% -----------------------------------------------------------------------------

\begin{document}

\maketitle

\section{Purpose}
	The purpose of this document is to record Phil's notes on information-theoretic methods for summarizing spatially-structured arrays of empirically-observed compositional data. 

	\subsection{Motivation}
		A spatially-organized summary of such a data set would provide candidate answers to each of the following questions: 
		\begin{enumerate}
			\item What is the ``scale'' of spatial heterogeneity in a city? For example, if Dorchester is substantially poorer than the rest of Boston, is that a `Dorchester-level' phenomenon? Or is it an amalgamation of smaller spatial phenomena that may need a separate or coordinated approach?
			\item Suppose we'd like to construct a generative theory of a spatially-distributed phenomenon based on some data. In the study of complex systems, we may not insist on a theory that perfectly reconstructs the data, but we should expect it to get the `high-level' features right. What are those features that such a theory should replicate?
			\item To what extent can a set of compositional data be explained by spatial effects? What phenomena in the data are decidedly ``nonspatial''?
			\item How complex is a city's distribution of income, as compared to other cities?
		\end{enumerate}
\section{Methods}
	\subsection{Data}
	Suppose that, for each $i = 1,\ldots,I$, we are given:
	\begin{enumerate}
		\item A vector $x^i\in \R^n$.
		\item A vector $\mathbf{p}^i \in \mathcal{S}^J$, where $\mathcal{S}^J = \{\mathbf{p} | \mathbf{p} \geq 0,\; \sum_{j = 1}^J p_j = 1\}$. Such data is sometimes referred to as ``compositional data'' because it reflects the composition of a whole, rather than individual magnitudes. 
	\end{enumerate}
	A natural class of such data are demographics organized by spatial tracts, for which $n = 2$. For example, the Census collects data on income by block group. For example, in the 2012 American Community Survey data for the city of Boston, the number of block groups $I = 646$ and the number of categories $J = 16$. Each $x^i \in \R^2$ would then be the centroid of the corresponding tract. 

	\subsection{Objective Function}
	We would like to generate, at each location $x^i$, a model prediction $\mathbf{q}^i$ of the data $\mathbf{p}^i$ that is in some sense ``spatially structured.'' Since $\mathbf{p}^i$ is compositional, a natural loss function is the Kullback-Leibler divergence of the estimate $\mathbf{q}^i$ from the observed data $\mathbf{p}^i$: 
	\begin{equation}
		D[\mathbf{p}^i\|\mathbf{q}^i] \triangleq \sum_{j=1}^J p^i_j \log \left(\frac{p^i_j}{q^i_j} \right)
	\end{equation}
	If 
	\begin{equation}
		\mathbf{P} = \left[\begin{array}{c c c}
			| & \cdots & | \\
			\mathbf{p^1} & \cdots & \mathbf{p}^I \\
			| & \cdots & | 
		\end{array}\right],
		\quad 
		\mathbf{Q} = \left[\begin{array}{c c c}
			| & \cdots & | \\
			\mathbf{q^1} & \cdots & \mathbf{q}^I \\
			| & \cdots & | 
		\end{array}\right],
	\end{equation}
	then we can write the total objective function for the problem as 
	\begin{equation}
		f_\mathbf{P}(\mathbf{Q}) = \sum_{i = 1}^I D[\mathbf{p}^i \| \mathbf{q}^i]\;.
	\end{equation}
	The objective function is interpretable as the number of additional bits needed to fully specify the true data $\mathbf{P}$ after approximating it with $\mathbf{Q}$. Of course, setting $\mathbf{Q} = \mathbf{P}$ achieves the lower bound $f_\mathbf{P}(\mathbf{P}) = 0$, but this achieves no modeling value. 

	\subsection{Spatial Structure}
		To impose spatial structure on our model, we require that our estimates $\mathbf{Q}$ have the structure 
		\begin{equation}
			\mathbf{q}^i = \mathbf{R} \mathbf{\Lambda}(x^i) \label{spatial}
		\end{equation}
		where $K \leq I$, 
		\begin{equation}
		\mathbf{R} = \left[\begin{array}{c c c}
			| & \cdots & | \\
			\mathbf{r}^1 & \cdots & \mathbf{r}^K \\
			| & \cdots & | 
		\end{array}\right] \in \R^{J \times K}
		\end{equation}
		is a matrix of ``representative'' distributions, and $\mathbf{\Lambda}: \R^n \rightarrow \R^K$ describes how the representative distributions $\mathbf{R}$ mix spatially. In order for $\mathbf{q^i}$ to be a valid probability distribution, we require that $\mathbf{\Lambda}(x^i) \geq 0$ and $\sum_{k = 1}^K \lambda_k(x^i) = 1$ for all $i$. Thus, \eqref{spatial} states that each $\mathbf{q}^i$ is a convex combination of the representative distributions $\{\mathbf{r}^k\}$, where the convex coefficients are determined by the spatial location $x^i$. 

		The modeling task is to find the matrix $\mathbf{R}$ and function $\mathbf{\Lambda}$. We need to restrict the set of candidate functions $\mathbf{\Lambda}$ such that: 
		\begin{enumerate}
		 	\item Elements of this set set reflect reasonably intuitive spatial structure. 
		 	\item Optimizing over this set is computationally feasible, which we take to imply finite-dimensional. 
		\end{enumerate} 
		A set of functions that fits the bill is: 
		\begin{equation}
			\mathbf{L} = \left\{ \Lambda(x) = \frac{\mathbf{c} \bullet \Phi(x|\Theta)}{\mathbf{c} \cdot \Phi(x|\Theta)},\; \Phi(x|\Theta) = (\phi(x|\theta^1),\ldots, \phi(x|\theta^K)),\; \Theta = (\theta^1,\ldots,\theta^K), \; \mathbf{c}\in \R^K_+ \right\},
		\end{equation}
		where $\bullet$ is the Hadamard product and $\phi(x|\theta)$ is the multivariate Gaussians distribution with parameters $\theta$ evaluated at $x$. The set $\mathbf{L}$ is an attractive setting for us because:
		\begin{enumerate}
		 	\item Elements of this set depend spatially on $x$ through unimodal distributions, reflecting the idea that the influence of each representative distribution $\mathbf{q}^i$ is ``centered'' at the mean of the corresponding density and decays with distance. Some may be more absolutely ``influential'' than others, and will have high corresponding entries of $\mathbf{c}$. 
		 	\item This set is finite dimensional: we need to fit $KJ$ entries of $\mathbf{R}$, $\frac{n(n+3)}{2}$ parameters for each of $K$ Gaussians, and $K$ parameters $\mathbf{c}$, giving a total problem dimension of $K\left(J +  \frac{n(n+3)}{2} + 1\right)$. 
		\end{enumerate}
	\subsection{Problem Statement}
		With this framework in place, we can define our optimization problem. First, since 
		\begin{equation}
			D[\mathbf{p}^i \| \mathbf{q}^i] = \sum_{j = 1}^J p^i_j \log \frac{p^i_j}{q^i_j} = \sum_{j = 1}^J p^i_j \log p^i_j - \sum_{j = 1}^J p^i_j \log q^i_j \;,
		\end{equation}
		we can define $g_i(\mathbf{q}^i) = - \mathbf{p}^i \cdot \log \mathbf{q}^i$ (the logarithm is evaluated componentwise) and minimize 
		\begin{equation}
			- \sum_{i = 1}^I g_i(\mathbf{q^i})\;.
		\end{equation}
		Then, if we define 
		\begin{equation}
			h_i(\mathbf{R}, \Theta, \mathbf{c}) \triangleq \mathbf{R} \frac{\mathbf{c} \bullet \Phi(x^i|\Theta)}{\mathbf{c} \cdot \Phi(x^i|\Theta)}\;,
 		\end{equation}
 		we can write our main optimization problem as 
 		\begin{equation}
	\begin{aligned}
		& \min_{\mathbf{R}, \Theta, \mathbf{C}} 
		& & \sum_i (g_i \circ h_i)(\mathbf{R}, \Theta, \mathbf{c})   \\
		& \text{subject to} 
		& & \mathbf{c} \geq 0 \\
		& & & \mathbf{r}^k \in \mathcal{S}^J \quad \forall k \;.
	\end{aligned} \label{opt}
	\end{equation}
	The problem \eqref{opt} has the following properties:  
	\begin{enumerate}
		\item The feasible region is convex. 
		\item The objective function is likely nonconvex.
		\item The objective function is smooth. 
	\end{enumerate}
	Thus, finding a local optimum would likely be easy with simple first- or second-order methods, but finding a global optimum could be nontrivial. 
\section{Open Questions}
	\begin{itemize}
		\item Is \eqref{opt} convex up to permutations of the indices $k = 1,\ldots,K$? This would imply that the only local optima are $K!$ global optima, which are identical up to relabeling the parameters. 
		\item How much ``signal'' should we expect in a standard data set? 
		\item How nearly-optimal are local minima? How bad would it be to ``settle'' for a local optimum?
	\end{itemize}
	








% We can now proceed to compute gradients. We have 
% \begin{align*}
% 	\nabla f_i (\mathbf{q}_i) = p_i \bullet q_i^{*}\;,
% \end{align*}
% where $q_i^{*}$ is the vector whose $j$th component is the reciprocal of the $j$th component of $q$. Furthermore, the components of $\nabla g_i$ are
% \begin{align}
% 	\frac{\partial g(\mathbf{R}, \mathbf{\Theta}, \mathbf{c})}{\partial c^k} &= \mathbf{R}\frac{(\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta}))\mathbf{e}_k \bullet \mathbf{\Lambda}(x_i|\mathbf{\Theta}) - (\mathbf{e}_k \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta})) \mathbf{c} \bullet \mathbf{\Lambda}(x_i|\mathbf{\Theta})}{(\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta}))^2} \\
% 	&= \mathbf{R}\frac{\left[\mathbf{\Lambda}(x_i|\mathbf{\Theta}) \cdot (\mathbf{c} - \mathbf{e}_k)\right]   \left[(\mathbf{e_k - \mathbf{c}}) \bullet \mathbf{\Lambda}(x_i|\mathbf{\Theta})\right]}{(\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta}))^2} \\
% 	\frac{\partial g(\mathbf{R}, \mathbf{\Theta}, \mathbf{c})}{\partial r^k_j} &=  \frac{ \mathbf{e}_k \cdot [\mathbf{c} \bullet \mathbf{\Lambda}(x_i|\mathbf{\Theta})] }{\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta})} \\
% 	\frac{\partial g(\mathbf{R}, \mathbf{\Theta}, \mathbf{c})}{\partial \theta^k_l} &= \mathbf{R} \frac{[\mathbf{e}_k \bullet \mathbf{c}] \frac{\partial \lambda(x_i|\theta^k)}{ \partial \theta^k_\ell}\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta}) - \mathbf{e}_k \cdot \mathbf{c} \frac{\partial \lambda(x_i |\theta^k)}{\partial \theta^k_\ell} \mathbf{c} \bullet \mathbf{\Lambda}(x_i|\mathbf{\Theta})}{(\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta}))^2} \\
% 	&= \mathbf{R} \frac{\mathbf{c} \bullet \left[ (\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta})) \mathbf{e}_k - (\mathbf{e}_k \cdot \mathbf{c})\mathbf{\Lambda}(x_i|\mathbf{\Theta}) \right]}{(\mathbf{c} \cdot \mathbf{\Lambda}(x_i|\mathbf{\Theta}))^2} \frac{\partial \lambda(x_i|\theta^k)}{ \partial \theta^k_\ell}\;.
% \end{align}
% The exact manner in which these gradients are organized depends on the formulation, but ultimately it's just a matter of re-ordering where needed. 




% \section{Helpful Citations}
\end{document}
